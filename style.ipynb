{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport torch\nimport os\nfrom PIL import Image \nfrom torchvision.transforms.functional import to_pil_image\nfrom torchvision.transforms import Resize ,Compose,Normalize,ToTensor\nimport torch.nn.functional as F\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\npath2data = '../input/styledata/'\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-12T11:13:08.747667Z","iopub.execute_input":"2021-12-12T11:13:08.748928Z","iopub.status.idle":"2021-12-12T11:13:10.527557Z","shell.execute_reply.started":"2021-12-12T11:13:08.748522Z","shell.execute_reply":"2021-12-12T11:13:10.526825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path2content = os.path.join(path2data,'content.jpg')\npath2style = os.path.join(path2data,'style.jpeg')\ncontent_img = Image.open(path2content)\nstyle_img  = Image.open(path2style)\n\nh, w = 256, 384\nVGG19_mean = torch.tensor([0.485, 0.456, 0.406])\nVGG19_std  = torch.tensor([0.229, 0.224, 0.225])\nimage_transform = Compose([\n    ToTensor(),\n    Resize(size=(h,w)),\n    Normalize(VGG19_mean,VGG19_std),\n])\nstyle_img = image_transform(style_img)\ncontent_img = image_transform(content_img)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T11:13:10.52939Z","iopub.execute_input":"2021-12-12T11:13:10.529665Z","iopub.status.idle":"2021-12-12T11:13:11.246426Z","shell.execute_reply.started":"2021-12-12T11:13:10.529633Z","shell.execute_reply":"2021-12-12T11:13:11.245657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tensorToPil(img):\n    image = img.clone().detach()\n    image *= VGG19_std.view(3,1,1)\n    image += VGG19_mean.view(3,1,1)\n    image = image.clamp(0,1)\n    return to_pil_image(image)\nfor i,title,img in zip(range(1,3,1),['style','content'],[style_img,content_img]):\n    ax = plt.subplot(1,2,i)\n    ax.imshow(tensorToPil(img))\n    ax.set_title(title)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-12T11:13:11.247752Z","iopub.execute_input":"2021-12-12T11:13:11.248004Z","iopub.status.idle":"2021-12-12T11:13:11.595707Z","shell.execute_reply.started":"2021-12-12T11:13:11.247967Z","shell.execute_reply":"2021-12-12T11:13:11.594417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.models as models\nmodel = models.vgg19(pretrained=True).to(device)\nmodel_vgg = model.features\nfor parameters in model_vgg.parameters():\n    parameters.requires_grad_(False)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T11:13:11.597543Z","iopub.execute_input":"2021-12-12T11:13:11.597788Z","iopub.status.idle":"2021-12-12T11:13:35.87123Z","shell.execute_reply.started":"2021-12-12T11:13:11.597757Z","shell.execute_reply":"2021-12-12T11:13:35.870406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_features(x,model,layers):\n    features = {}\n    for name,layer in enumerate(model.children()):\n        x = layer(x)\n        if name in layers:\n            features[int(name)] = x\n    return features\ndic=get_features(content_img.unsqueeze(0),model_vgg,[0,5,11,19,28])\nprint(type(dic))\nfor x in dic:\n    print(dic[x].shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T11:13:35.872599Z","iopub.execute_input":"2021-12-12T11:13:35.873075Z","iopub.status.idle":"2021-12-12T11:13:36.058292Z","shell.execute_reply.started":"2021-12-12T11:13:35.873037Z","shell.execute_reply":"2021-12-12T11:13:36.057084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gram_matrix(x):\n    n,c,h,w = x.size()\n    x = x.view(c,h*w)\n    x = torch.mm(x,x.t())\n    return x\ngram_matrix(style_img.unsqueeze(0))","metadata":{"execution":{"iopub.status.busy":"2021-12-12T11:13:36.059468Z","iopub.status.idle":"2021-12-12T11:13:36.065754Z","shell.execute_reply.started":"2021-12-12T11:13:36.065488Z","shell.execute_reply":"2021-12-12T11:13:36.065516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def content_loss(pred_features,target_features,layer):\n   \n    pred = gram_matrix(pred_features[layer])\n    target = gram_matrix(target_features[layer])\n    loss = F.mse_loss(pred,target)\n    return loss\na = get_features(style_img.unsqueeze(0),model_vgg,layers = [0,5,10,19,21,28])\n","metadata":{"execution":{"iopub.status.busy":"2021-12-12T11:13:36.06702Z","iopub.status.idle":"2021-12-12T11:13:36.067581Z","shell.execute_reply.started":"2021-12-12T11:13:36.067354Z","shell.execute_reply":"2021-12-12T11:13:36.067379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def style_loss(pred_f,target_f,layers_dic):\n    loss = 0\n    for layer,weight in zip(layers_dic,[0.75,0.5,.25,.25,.25]):\n        n,c,h,w = pred_f[layer].shape\n        pred = gram_matrix(pred_f[layer])\n        target = gram_matrix(target_f[layer])\n        loss += weight*(F.mse_loss(pred,target)/(n*c*h*w))\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-12-12T11:14:39.43894Z","iopub.execute_input":"2021-12-12T11:14:39.439541Z","iopub.status.idle":"2021-12-12T11:14:39.445504Z","shell.execute_reply.started":"2021-12-12T11:14:39.439494Z","shell.execute_reply":"2021-12-12T11:14:39.444539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"content_features = get_features(content_img.unsqueeze(0),model_vgg,layers_)\nfor key in content_features.keys():\n    print(content_features[key].shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T11:13:36.074868Z","iopub.status.idle":"2021-12-12T11:13:36.075266Z","shell.execute_reply.started":"2021-12-12T11:13:36.075031Z","shell.execute_reply":"2021-12-12T11:13:36.075051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.optim import Adam\ninput_tensor = content_img.clone().requires_grad_(True)\noptim = Adam([input_tensor], lr=0.01)\nlayers_ = [0,5,10,19,21,28]\nnumber_of_epoch = 300\nfor i in range(number_of_epoch):\n    optim.zero_grad()\n    content_features = get_features(content_img.unsqueeze(0),model_vgg,layers_)\n    style_features = get_features(style_img.unsqueeze(0),model_vgg,layers_)\n    input_features = get_features(input_tensor.unsqueeze(0),model_vgg,layers_)\n    \n    style_s = style_loss(input_features,style_features,layers_)\n    content_s = content_loss(input_features,content_features,28)\n    print(content_s)\n    print(style_s)\n    loss = 1e4*style_s + 1e1*content_s\n    loss.backward(retain_graph=True)\n    optim.step()\n\ndiff = input_tensor-content_img\nprint(torch.sum(diff))\nplt.imshow(tensorToPil(input_tensor))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-12T11:15:47.449784Z","iopub.execute_input":"2021-12-12T11:15:47.450288Z","iopub.status.idle":"2021-12-12T11:15:52.21201Z","shell.execute_reply.started":"2021-12-12T11:15:47.450243Z","shell.execute_reply":"2021-12-12T11:15:52.211058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}